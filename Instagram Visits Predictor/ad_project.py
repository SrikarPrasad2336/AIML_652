# -*- coding: utf-8 -*-
"""AD_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tQDwnUv1bDSkKcjfSr2DeDQIh1aAKJsR
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
file_path = '/content/drive/MyDrive/Colab Notebooks/instagram_visits_prediction_large.csv'
df = pd.read_csv(file_path)
print(df.info())
print(df.head())
print(df.describe())
label_encoders = {}
for col in ['Post Type', 'Time of Day']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le
X_cluster = df[['Follower Count', 'Avg Likes']]
kmeans = KMeans(n_clusters=3, random_state=42)
df['Cluster'] = kmeans.fit_predict(X_cluster)
plt.figure(figsize=(8, 6))
sns.scatterplot(x='Follower Count', y='Avg Likes', hue='Cluster', data=df, palette='viridis')
plt.title('K-Means Clustering on Follower Count vs Avg Likes')
plt.show()
X_lr = df[['Follower Count', 'Hashtags Count', 'Caption Length', 'Avg Likes', 'Avg Comments']]
y_lr = df['Predicted Visits']

lr_model = LinearRegression()
lr_model.fit(X_lr, y_lr)
plt.figure(figsize=(8, 6))
sns.regplot(x=df['Avg Likes'], y=df['Predicted Visits'], line_kws={"color": "red"})
plt.title('Linear Regression: Avg Likes vs Predicted Visits')
plt.show()

# Feature Engineering (Example: Interaction Term)
df['Likes_Comments_Ratio'] = df['Avg Likes'] / (df['Avg Comments'] + 1)
rfe = RFE(estimator=lr_model, n_features_to_select=3)
rfe.fit(X_lr, y_lr)
print("Selected Features (RFE):", X_lr.columns[rfe.support_])
X_dt = df[['Follower Count', 'Hashtags Count', 'Post Type', 'Time of Day', 'Caption Length']]
y_dt = (df['Predicted Visits'] > df['Predicted Visits'].median()).astype(int)  # Binary target
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_dt, y_dt)
print("Decision Tree Classifier trained successfully")

# Re-importing necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier, plot_tree

# Load dataset
file_path = '/content/drive/MyDrive/Colab Notebooks/instagram_visits_prediction_large.csv'
df = pd.read_csv(file_path)

# Encode categorical variables
label_encoders = {}
for col in ['Post Type', 'Time of Day']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

# Prepare data for Decision Tree Classifier
X_dt = df[['Follower Count', 'Hashtags Count', 'Post Type', 'Time of Day', 'Caption Length']]
y_dt = (df['Predicted Visits'] > df['Predicted Visits'].median()).astype(int)  # Binary target

# Train Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_dt, y_dt)

from sklearn.metrics import r2_score, mean_squared_error
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
import numpy as np

file_path = '/content/drive/MyDrive/Colab Notebooks/instagram_visits_prediction_large.csv'
data = pd.read_csv(file_path)
# Drop the 'UserID' column (not useful for prediction)
data.drop('UserID', axis=1, inplace=True)

# Encode categorical features ('Post Type' and 'Time of Day')
for col in ['Post Type', 'Time of Day']:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])

# Separate features and target variable
X = data.drop('Predicted Visits', axis=1)
y = data['Predicted Visits']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Decision Tree Regressor
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions
y_pred = dt_model.predict(X_test)

# Compute evaluation metrics
r2 = r2_score(y_test, y_pred)
n, p = X_test.shape  # Number of samples, Number of features
adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1))
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Display results
r2, adjusted_r2, mse, rmse

print(f" R^2 Score: {r2:.5f}")
print(f" Adjusted R^2 Score: {adjusted_r2:.5f}")
print(f" Mean Squared Error (MSE): {mse:.2f}")
print(f" Root Mean Squared Error (RMSE): {rmse:.2f}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
threshold = y.median()
y_train_class = (y_train >= threshold).astype(int)
y_test_class = (y_test >= threshold).astype(int)
y_pred_class = (y_pred >= threshold).astype(int)
accuracy = accuracy_score(y_test_class, y_pred_class)
precision = precision_score(y_test_class, y_pred_class)
recall = recall_score(y_test_class, y_pred_class)
f1 = f1_score(y_test_class, y_pred_class)
print(f" Accuracy: {accuracy:.5f}")
print(f" Precision: {precision:.5f}")
print(f" Recall: {recall:.5f}")
print(f" F1-Score: {f1:.5f}")

